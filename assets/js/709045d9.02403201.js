"use strict";(globalThis.webpackChunkexperimenter_docs=globalThis.webpackChunkexperimenter_docs||[]).push([[6717],{5680(e,t,n){n.d(t,{xA:()=>p,yg:()=>u});var a=n(6540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),m=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=m(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),c=m(n),u=r,h=c["".concat(l,".").concat(u)]||c[u]||d[u]||i;return n?a.createElement(h,s(s({ref:t},p),{},{components:n})):a.createElement(h,s({ref:t},p))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,s=new Array(i);s[0]=c;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:r,s[1]=o;for(var m=2;m<i;m++)s[m]=n[m];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},704(e,t,n){n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>m});var a=n(8168),r=(n(6540),n(5680));const i={id:"overview",title:"Overview",slug:"/data-analysis/jetstream/overview",sidebar_position:1},s=void 0,o={unversionedId:"data-analysis/jetstream/overview",id:"data-analysis/jetstream/overview",title:"Overview",description:"[Jetstream] is an analysis framework for experiments.",source:"@site/docs/data-analysis/jetstream/overview.md",sourceDirName:"data-analysis/jetstream",slug:"/data-analysis/jetstream/overview",permalink:"/data-analysis/jetstream/overview",draft:!1,editUrl:"https://github.com/mozilla/experimenter-docs/edit/main/docs/data-analysis/jetstream/overview.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"overview",title:"Overview",slug:"/data-analysis/jetstream/overview",sidebar_position:1},sidebar:"sidebar",previous:{title:"Developer Tools",permalink:"/resources/nimbus-devtools-guide"},next:{title:"Metrics",permalink:"/data-analysis/jetstream/metrics"}},l={},m=[{value:"Analysis Paradigm",id:"analysis-paradigm",level:2},{value:"Enrollment vs Exposure",id:"enrollment-vs-exposure",level:2},{value:"Analysis Steps",id:"analysis-steps",level:2},{value:"Tooling and Metric Versioning",id:"tooling-and-metric-versioning",level:2},{value:"How to Use the Latest Tooling and Metric Definitions?",id:"how-to-use-the-latest-tooling-and-metric-definitions",level:3},{value:"Datasets",id:"datasets",level:2}],p={toc:m};function d(e){let{components:t,...n}=e;return(0,r.yg)("wrapper",(0,a.A)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("a",{parentName:"strong",href:"https://github.com/mozilla/jetstream"},"Jetstream"))," is an analysis framework for experiments.\nJetstream aggregates and summarizes product telemetry,\nusing the experiment definitions in Experimenter,\nto produce the datasets that drive the results dashboards."),(0,r.yg)("p",null,"Most investigation owners will not interact with Jetstream directly.\nJetstream computes a default set of statistics for every experiment.\nInvestigation owners can add additional ",(0,r.yg)("a",{parentName:"p",href:"./metrics"},"metrics")," to a results dashboard by choosing\n",(0,r.yg)("a",{parentName:"p",href:"./outcomes"},"outcome"),"s in Experimenter while designing an experiment."),(0,r.yg)("p",null,"Data scientists can extend Jetstream with new outcomes by contributing\n",(0,r.yg)("a",{parentName:"p",href:"./outcomes"},"outcome")," definitions\nto the ",(0,r.yg)("inlineCode",{parentName:"p"},"jetstream/")," directory in the ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/mozilla/metric-hub/tree/main/jetstream"},"metric-hub")," repository."),(0,r.yg)("p",null,"Data scientists can also ask Jetstream to evaluate custom metrics for a particular experiment\nby contributing ",(0,r.yg)("a",{parentName:"p",href:"./configuration"},"experiment configurations"),"\nto metric-hub."),(0,r.yg)("p",null,"Jetstream is not a monitoring platform,\nwhich means that Jetstream does not emit real-time results.\nThe first interesting results will usually be available\na week after the enrollment period ends.\nTypically, that means results will begin to appear\ntwo weeks after the day the experiment launches."),(0,r.yg)("h2",{id:"analysis-paradigm"},"Analysis Paradigm"),(0,r.yg)("p",null,"Experiments are analyzed using the concept of analysis windows. Analysis\nwindows describe an interval marked from each client\u2019s day of\nenrollment. The \u201cday 0\u201d analysis window aggregates data from the days\nthat each client enrolled in the experiment. Because the intervals are\ndemarcated from enrollment, they are not calendar dates; for some\nclients in an experiment, day 0 could be a Tuesday, and for others a\nSaturday."),(0,r.yg)("p",null,"The week 0 analysis window aggregates data from each client\u2019s days 0\nthrough 6, the week 1 window aggregates data from days 7 through 13, and\nso on."),(0,r.yg)("p",null,"Clients are given a fixed amount of time, specified in Experimenter and\noften a week long, to enroll. Final day 0 results are available for\nreporting at the end of the enrollment period, after the last eligible\nclient has enrolled, and week 0 results are available a week after the\nenrollment period closes. Results for each window are published as soon\nas complete data is available for all enrolled clients."),(0,r.yg)("p",null,'The "overall" window, published after the experiment has ended, is a\nwindow beginning on each client\u2019s day 0 that spans the longest period\nfor which all clients have complete data.'),(0,r.yg)("h2",{id:"enrollment-vs-exposure"},"Enrollment vs Exposure"),(0,r.yg)("p",null,"Enrollment and exposure are two separate steps in the experiment lifecycle.\nAll experiments have enrollment events; some experiments have exposure events."),(0,r.yg)("p",null,(0,r.yg)("em",{parentName:"p"},"Enrollment")," happens when a Firefox client receives a recipe,\nevaluates the targeting expression,\nand decides to enroll in the experiment."),(0,r.yg)("p",null,(0,r.yg)("em",{parentName:"p"},"Exposure")," occurs at the earliest moment at which a user would encounter a different experience\nin different branches of the experiment."),(0,r.yg)("p",null,"Not all enrolled users will be exposed, though all exposed users must have been enrolled.\nCalculating metrics based on ",(0,r.yg)("em",{parentName:"p"},"enrolled")," users will tell us what the effect on our KPIs for the targeted segment would be if we deployed the treatment to everyone.\nCalculating metrics based on ",(0,r.yg)("em",{parentName:"p"},"exposed")," users helps us understand whether we had an effect on the users we were able to reach.\nUsing an exposure basis for our analysis helps us understand if we are having an interesting effect on a small population."),(0,r.yg)("p",null,"For example, consider an experiment on the Picture in Picture feature.\nPiP displays a small icon when users hover their mouse over an eligible video.\nIf a user never visits a page with an eligible video, the experiment cannot have any effect on them.\nTwo reasonable choices for exposure events could be the moment that a page is loaded with an eligible video,\nor the moment that the PiP overlay icon is first displayed."),(0,r.yg)("p",null,"Any telemetry collection can be used as an exposure event, though events are often especially useful.\nMany Nimbus features will send a ",(0,r.yg)("a",{parentName:"p",href:"/technical-reference/fml/fml-spec#recording-exposure"},"Nimbus exposure event")," automatically when the feature configuration is consulted;\nthese are ",(0,r.yg)("inlineCode",{parentName:"p"},"normandy#expose")," events on desktop and ",(0,r.yg)("inlineCode",{parentName:"p"},"nimbus_events.exposure")," events in Glean."),(0,r.yg)("h2",{id:"analysis-steps"},"Analysis Steps"),(0,r.yg)("p",null,"When analyzing experiments, the following steps are executed for each experiment:"),(0,r.yg)("img",{src:"/img/jetstream/analysis-steps.png",alt:"Experiment analyis steps",className:"img-sm"}),(0,r.yg)("p",null,"A ",(0,r.yg)("a",{parentName:"p",href:"https://mozilla.github.io/metric-hub/default_configs/firefox_desktop/"},"default configuration"),"\nwhich depends on the experiment type and, if defined, a custom configuration\nprovided via the ",(0,r.yg)("inlineCode",{parentName:"p"},"jetstream/")," directory in the ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/mozilla/metric-hub/tree/main/jetstream"},"metric-hub")," repository are parsed and used for analysis.\nThe experiment definition and config parameters are used to run some checks\nto determine if the experiment can be analyzed. These checks include, for example,\nvalidating start dates, end dates and enrollment periods."),(0,r.yg)("p",null,"If the experiment is valid, then metrics are calculated for each analysis period\n(daily, weekly, 28 days, overall) and written to BigQuery. Metrics are either\nspecified or a reference to existing metrics defined in ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/mozilla/mozanalysis"},"mozanalysis"),"\nis provided in the configuration files. Next, for each segment, first\n",(0,r.yg)("a",{parentName:"p",href:"https://github.com/mozilla/jetstream/blob/main/jetstream/pre_treatment.py"},"pre-treatments"),"\nare applied to the metrics data which is then used to calculate\n",(0,r.yg)("a",{parentName:"p",href:"https://github.com/mozilla/jetstream/blob/main/jetstream/statistics.py"},"statistics"),".\nStatistics data is written to BigQuery and later exported to GCS as JSON. "),(0,r.yg)("h2",{id:"tooling-and-metric-versioning"},"Tooling and Metric Versioning"),(0,r.yg)("p",null,"Jetstream ensures that results get computed consistently across the entire analysis duration of an experiment.\nIt prevents sudden changes of how results are computed after tooling (such as ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/mozilla/mozanalysis"},"mozanalysis")," which Jetstream depends on) or default metrics get updated by using the same versions as when the analysis initially started until the experiment completes."),(0,r.yg)("h3",{id:"how-to-use-the-latest-tooling-and-metric-definitions"},"How to Use the Latest Tooling and Metric Definitions?"),(0,r.yg)("p",null,"When tooling or metrics are updated, there are a few options with different consequences for how Jetstream treats existing experiments:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"do nothing"),": only experiments that are launched after the new tooling release will use the most recent version of the tooling "),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"use new tooling version without rerunning"),": install the ",(0,r.yg)("a",{parentName:"li",href:"https://pypi.org/project/mozilla-jetstream/"},"jetstream command line tooling")," locally and run ",(0,r.yg)("inlineCode",{parentName:"li"},"jetstream rerun-skip --experiment_slug=<slug>"),". This command pretends to rerun the experiment without actually re-running the queries, which will update the date used to determine which tooling and metric versions to use. Rerunning an experiment will always force new versions to be used for experiments. "),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"rerun experiment"),": re-running an experiment will always use the most recent version of the tooling on the rerun and update the last updated timestamps of the result tables")),(0,r.yg)("p",null,"More detailled information around versioning is available in ",(0,r.yg)("a",{parentName:"p",href:"./operations#tooling-and-metric-versioning"},"Jetstream Operations"),"."),(0,r.yg)("h2",{id:"datasets"},"Datasets"),(0,r.yg)("p",null,"The datasets that back the Experimenter results dashboards\nare available in BigQuery.\n",(0,r.yg)("a",{parentName:"p",href:"https://docs.telemetry.mozilla.org/datasets/jetstream.html"},"Technical documentation"),"\nis available in the Mozilla data docs."))}d.isMDXComponent=!0}}]);